# -*- coding: utf-8 -*-
"""mistralLLM-Chatbot Prac

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14tP_pTVxe8W0PLaBKpYRGcPNOcOiCAtR
"""

# installing dependencies
!pip install accelerate peft bitsandbytes git+https://github.com/huggingface/transformers trl py7zr auto-gptq optimum

# getting token from hugging face access token
from huggingface_hub import notebook_login
notebook_login()

# we using torch framework, data set class provides functionality
# in peft we basically fine tune our model

import torch
from datasets import load_dataset, Dataset
from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model
from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig, TrainingArguments
from trl import SFTTrainer
import os

data = load_dataset("tatsu-lab/alpaca", split="train")
data_df = data.to_pandas()
data_df = data_df[:5000]
data_df["text"] = data_df[["input", "instruction", "output"]].apply(lambda x: "###Human: " + x["instruction"] + " " + x["input"] + " ###Assistant: "+ x["output"], axis=1)
data = Dataset.from_pandas(data_df)


tokenizer = AutoTokenizer.from_pretrained("TheBloke/Mistral-7B-Instruct-v0.1-GPTQ")
tokenizer.pad_token = tokenizer.eos_token

quantization_config_loading = GPTQConfig(bits=4, disable_exllama=True, tokenizer=tokenizer)
model = AutoModelForCausalLM.from_pretrained(
                            "TheBloke/Mistral-7B-Instruct-v0.1-GPTQ",
                            quantization_config=quantization_config_loading,
                            device_map="auto"
                        )


model.config.use_cache=False
model.config.pretraining_tp=1
model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)

# Low-Rank Adaptation of Large Language Models
# r is rank (lesser the better) [r=16 is scalable]
# q_proj and v_proj are attention layers

peft_config = LoraConfig(
    r=16, lora_alpha=16, lora_dropout=0.05, bias="none", task_type="CAUSAL_LM", target_modules=["q_proj", "v_proj"]
)
model = get_peft_model(model, peft_config)

training_arguments = TrainingArguments(
        output_dir="mistral-finetuned-alpaca",
        per_device_train_batch_size=8,
        gradient_accumulation_steps=1,
        optim="paged_adamw_32bit",
        learning_rate=2e-4,
        lr_scheduler_type="cosine",
        save_strategy="epoch",
        logging_steps=100,
        num_train_epochs=1,
        max_steps=250,
        fp16=True,
        push_to_hub=True
)

from trl import SFTTrainer

# Initialize SFTTrainer with required parameters
trainer = SFTTrainer(
    model=model,
    train_dataset=data,
    peft_config=peft_config,
    args=training_arguments,
    tokenizer=tokenizer,
    packing=False,
    max_seq_length=512,  # Set max_seq_length directly
    dataset_text_field="text"  # Set dataset_text_field directly
)

# Set tokenizer padding side
tokenizer.padding_side = 'right'

trainer.train()

! cp -r /content/mistral-finetuned-alpaca /content/drive/MyDrive/

from google.colab import drive
drive.mount('/content/drive')

!mkdir -p /content/drive/MyDrive/

!cp -r /content/mistral-finetuned-alpaca /content/drive/MyDrive/

"""# Trying Streamlit"""

import streamlit as st
from transformers import AutoTokenizer
from peft import AutoPeftModelForCausalLM
import torch

!pip install streamlit pyngrok

import streamlit as st
from transformers import AutoTokenizer
from peft import AutoPeftModelForCausalLM
import torch

pip install streamlit transformers peft torch
